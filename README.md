# HW_NLP

## Общее описание
Состоит из трёх файлов:
 * teach_fasttext.ipynb -- Предобучение FastText
 * NLP_with_fasttext.ipynb -- собственно обучение модели и генерация предсказаний
 * fortune_telling.ipynb -- NLP гадание
 
 Использую LSTM + MLP для того, чтобы предсказывать значение. В качестве эмбедингов использую FastText вектора, обученные на пяти датасетах, полученных из телеграма/интернета.
 
 ## Использование дополнительных данных
Предобучил векторные эмбеддинги (gensim.FastText) на списке из пяти датасетов:
* Трейновые предложения. Они нужны, чтобы моделька точно выучила частые слова из имеющихся данных

* 3 датасета, похожих на каждую группу:
   * Датасет с диалогами между подростками (https://github.com/Phylliida/Dialogue-Datasets). Похоже на класс 0
   * Датасет с постами из телеграм-канала socryptoland. Похоже на класс 1
   * Датасет с постами из телеграм-канала inVogaFashion (единственный англоязычный канал про моду, который смог найти =))). Похоже на класс 2
* Часть датасета английской википедии (https://huggingface.co/datasets/wikipedia)

Предобрабатывал я всё тем же самописным NLP классом, который написал для предсказания. Он разбивал по словам, знаки препинания оставлял только избранные, также сохранял эмодзи и удалял повторяющиеся пробелы.

Данные из каналов скачивал самостоятельно, оставлял только подходящие по формату фразы (например, выкидывал короткие)
Предобрабатывал я всё тем же самописным NLP классом, который написал для предсказания. Он разбивал по словам, знаки препинания оставлял только избранные, также сохранял эмодзи и удалял повторяющиеся пробелы.

## Предсказание будущего по скрытому пространству
Получился такой результат:

```
1. Clarification from my friend.
2. my bad
3. My face, when always.
4. oh my
5. Separately, I will note my favorite way of 
```

От будущего довольно мало, скорее my... 
Но первое выглядит интересно, я стараюсь прислушиваться к советам друзей =)
